{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f4422e-ee32-4107-8893-53a415b61806",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8339c139-f7a9-4f1d-b500-e6bb985d5a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jnmel\\.conda\\envs\\meu_ambiente\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "face9722-0e91-4a97-8055-aba64c09d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938edf37-b102-4b0e-a30f-91c8af43d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory,ChatMessageHistory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5011e6-afa9-40d6-a0ac-9279cd84d855",
   "metadata": {},
   "source": [
    "# Loading Knowlege Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7ba606-d504-4641-9136-98fc45d1f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add metadata\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "# Adjusting utf-8 \n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "# Path to folder with the files\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "# Reading files\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24bfb06e-34eb-412a-b4c9-697c8ecb0a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1009, which is longer than the specified 800\n",
      "Created a chunk of size 1225, which is longer than the specified 800\n",
      "Created a chunk of size 890, which is longer than the specified 800\n",
      "Created a chunk of size 1302, which is longer than the specified 800\n",
      "Created a chunk of size 1318, which is longer than the specified 800\n",
      "Created a chunk of size 1338, which is longer than the specified 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 57\n",
      "Document types found: {'model', 'motoboys', 'company', 'services'}\n"
     ]
    }
   ],
   "source": [
    "# Creating chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=160)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97e1ff-dc7a-4706-8725-5dabb1e94c0d",
   "metadata": {},
   "source": [
    "# Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b028b9-5979-4d7f-a3f5-cb5245a66734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jnmel\\AppData\\Local\\Temp\\ipykernel_20960\\1120628988.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n"
     ]
    }
   ],
   "source": [
    "# Embeddings open source\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "746a495a-5a30-4299-a0ae-8ebec5c15957",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed8bfc89-9e59-476c-98da-ee095509bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete if already exists\n",
    "if os.path.exists(db_name):\n",
    "    shutil.rmtree(db_name)\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1052e84-d9a5-46f7-965e-a7dc02c7cd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 57 documents\n"
     ]
    }
   ],
   "source": [
    "# Create vectorstore\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1bc03ba-51cc-4fab-855e-db6108d1b25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57 vectors with 384 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "# The vectors\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c303e9c-6535-4b41-b451-ef611160e738",
   "metadata": {},
   "source": [
    "# Visualizing the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2694423e-dcc7-4bba-9d49-a6ad50d4b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework \n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "doc_types = [metadata['doc_type'] for metadata in metadatas]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['model', 'motoboys', 'services', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9826b15a-fa70-4be3-9d44-68f69afa5bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"820px\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_18.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"iframe_connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b70c20-41e8-4bb9-9ea5-456846d744c4",
   "metadata": {},
   "source": [
    "The figure visualizes the distribution of high-dimensional document embeddings in a 2D space using t-SNE. Each point represents a document, and its color indicates its category. Points that appear closer together are likely to be semantically similar, helping us identify clusters and relationships between categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84322ef0-ec46-49b3-96b1-7ca0f6cc33ca",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c98ffbbd-4168-4c82-8b1b-0334ecd433ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jnmel\\AppData\\Local\\Temp\\ipykernel_20960\\1052701110.py:7: LangChainDeprecationWarning:\n",
      "\n",
      "Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"llama3.2\"\n",
    "\n",
    "# create a new Chat with llama3.2\n",
    "llm = ChatOpenAI(temperature=0.7, model_name='llama3.2', base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888520b9-abd8-4ddd-a35d-c274951bb37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a simple question\n",
    "\n",
    "query = \"Please explain the System Flow for Demand Forecasting process in a couple of sentences\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0b399-6657-4b48-9846-5ce4158b9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a simple question\n",
    "query = \"Please tell me all the endpoints used in all process\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "display(Markdown(result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469752d8-516b-4c29-a153-9dff7d95aa98",
   "metadata": {},
   "source": [
    "# Chat Interface - Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "813e0b3f-14cb-4b2e-8c9e-f7e0f4c00e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1fc278a-921a-4cbb-89d7-e471e59bbe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3a844-c208-4bc8-a132-f25579b757e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
